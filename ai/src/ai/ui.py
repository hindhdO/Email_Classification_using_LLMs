import streamlit as st
from main import run, run_test, compute_metrics
import json

st.set_page_config(page_title="Email Classifier", layout="centered")
st.title("ğŸ“§ Smart Email Classifier")

st.markdown("Classify and evaluate emails using reasoning from large language models.")
st.markdown("---")

email = st.chat_input("ğŸ’¬ Enter the email you want to classify:")

if email:
    with st.spinner("ğŸ”„ Running classification and evaluation..."):
        result = run(email)

        classification = result.get("refinement_output") or json.loads(result.get("classification_output"))
        evaluation = json.loads(result.get("evaluation_output"))

    st.markdown("## âœ… Final Verdict")

    if classification:
        col1, col2 = st.columns(2)
        with col1:
            st.metric(label="Spam?", value="ğŸš« Yes" if classification.get("spam") else "ğŸ“© No")
        with col2:
            st.metric(label="Category", value=classification.get("category"))

        with st.expander("ğŸ” Classifier Reasoning", expanded=True):
            st.json(classification)
    else:
        st.warning("âš ï¸ No classification result available.")

    st.markdown("---")

    if evaluation:
        with st.expander("ğŸ§  Evaluator Analysis", expanded=True):
            st.json(evaluation)

        if evaluation.get("verdict") in ["Incorrect", "Partially correct"]:
            st.warning("ğŸ” The evaluator disagreed â€” refinement was applied.")
        else:
            st.success("âœ… Evaluator agrees with the classification. No refinement needed.")
    else:
        st.warning("âš ï¸ No evaluator output available.")

num_tests = st.number_input(
    "How many emails to generate?",
    min_value=1,
    value=10,  
    step=1
)

if st.button("Launch test cases"):
    with st.spinner("ğŸ”„ Running classification and evaluation..."):
        test_result = run_test(num_tests)

    test_output_raw = test_result.get("test_output")
    parsed_test_output = json.loads(test_output_raw)
        
    test_cases = parsed_test_output.get("tests", [])
    score = parsed_test_output.get("score")
    suggestion = parsed_test_output.get("suggestion")

    metrics = compute_metrics(test_cases)

    st.subheader("ğŸ“Š Metrics")

    ignore_keys = ["accuracy", "macro avg", "weighted avg"]
    for category, values in metrics.items():
        if category in ["Spam", "Urgent", "Not Urgent", "To Read"]:
            with st.expander(f"ğŸ“§ {category}"):
                col1, col2, col3 = st.columns(3)
                col1.metric("ğŸ¯ Precision", f"{values['precision']*100:.1f}%")
                col2.metric("ğŸ¯ Recall", f"{values['recall']*100:.1f}%")
                col3.metric("ğŸ§® F1-score", f"{values['f1-score']*100:.1f}%")

    if score is not None:
        st.success(f"âœ… accuracy: {score}/{num_tests}")

    if suggestion:
        st.info(f"ğŸ’¡ Suggestion: {suggestion}")

    if test_cases:
        st.subheader("ğŸ“„ Test Cases")
        for case in test_cases:
            with st.expander(f"Test: {case.get('expected')} â†’ {case.get('predicted')}"):
                st.write(case)
